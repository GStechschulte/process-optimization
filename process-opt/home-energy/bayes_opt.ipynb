{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import botorch\n",
    "import gpytorch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas\n",
    "\n",
    "- Start with two dimensions and then scale from there (done)\n",
    "- ARD kernel (done)\n",
    "- Use batch BayesOpt (PoI done)\n",
    "- Use a cost function so that energy consumption can not be less than 1\n",
    "- Implement a non-fixed error term, i.e. allow error to vary (distributional regression)\n",
    "- Compare models based on gpytorch metrics and mll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_energy = pd.read_excel(\"../../data/ENB2012_data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [\n",
    "    \"Relative Compactness\", \"Surface Area\", \"Wall Area\", \n",
    "    \"Roof Area\", \"Overall Height\", \"Orientation\", \n",
    "    \"Glazing Area\", \"Glazing Area Distribution\", \"Heating Load\", \n",
    "    \"Cooling Load\"\n",
    "]\n",
    "\n",
    "building_energy.columns = feature_names\n",
    "building_energy.columns = (building_energy.columns\n",
    "                           .str.replace(' ', '_')\n",
    "                           .str.lower()\n",
    "                        )\n",
    "\n",
    "building_energy[\"energy_consumption\"] = building_energy[[\"heating_load\", \"cooling_load\"]].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = building_energy.drop([\"heating_load\", \"cooling_load\", \"energy_consumption\"], axis=1)\n",
    "y = building_energy[\"energy_consumption\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_energy.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairplot of the features\n",
    "# sns.pairplot(building_energy, diag_kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_energy.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian process regression\n",
    "\n",
    "With two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = building_energy[[\"relative_compactness\", \"surface_area\"]]\n",
    "y = building_energy[\"energy_consumption\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = torch.tensor(min_max_scaler.fit_transform(X_train), dtype=torch.float)\n",
    "before = X_train_scaled.clone().detach()\n",
    "X_test_scaled = torch.tensor(min_max_scaler.transform(X_test), dtype=torch.float)\n",
    "\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.float)\n",
    "y_test = torch.tensor(y_test.values, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GP(gpytorch.models.ExactGP, botorch.models.gpytorch.GPyTorchModel):\n",
    "    num_outputs = 1\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GP, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(\n",
    "                ard_num_dims=train_x.shape[1]\n",
    "                )\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(gp_model, num_iters=100):\n",
    "    noise = 1e-4\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = gp_model(X_train_scaled, y_train, likelihood)\n",
    "    model.likelihood.noise = noise\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    loss_hist = []\n",
    "    for _ in tqdm(range(num_iters)):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_train_scaled)\n",
    "        loss = -mll(output, y_train)\n",
    "        loss_hist.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return model, likelihood, loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, likelihood, loss_hist = fit_model(GP, num_iters=100)\n",
    "plt.figure(figsize=(7, 3))\n",
    "plt.plot(torch.arange(len(loss_hist)), loss_hist);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = torch.linspace(0, 1, 100)\n",
    "grid_x1, grid_x2 = torch.meshgrid(grid, grid, indexing='ij')\n",
    "test_x = torch.stack([grid_x1.flatten(), grid_x2.flatten()], axis=1)\n",
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictive_dist = likelihood(model(test_x))\n",
    "    predictive_mean = predictive_dist.mean\n",
    "    predictive_std = predictive_dist.stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "\n",
    "c = ax[0].imshow(\n",
    "    predictive_mean.detach().reshape(100, 100).transpose(-1, -2),\n",
    "    origin=\"lower\",\n",
    "    extent=(0, 1, 0, 1),\n",
    ")\n",
    "ax[0].set_title(\"Predicted energy consumption $\\\\mu$\")\n",
    "plt.colorbar(c, ax=ax[0]);\n",
    "\n",
    "c = ax[1].imshow(\n",
    "    predictive_std.detach().reshape(100, 100).transpose(-1, -2),\n",
    "    origin=\"lower\",\n",
    "    extent=(0, 1, 0, 1),\n",
    ")\n",
    "plt.colorbar(c, ax=ax[1])\n",
    "ax[1].set_title(\"Predicted energy consumption $\\\\sigma$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BayesOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = building_energy[\n",
    "    [\n",
    "        \"relative_compactness\", \"surface_area\", \"wall_area\", \n",
    "        \"roof_area\", \"overall_height\", \"orientation\", \n",
    "        \"glazing_area\", \"glazing_area_distribution\"\n",
    "     ]\n",
    "    ]\n",
    "y = building_energy[\"energy_consumption\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "std_scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = torch.tensor(min_max_scaler.fit_transform(X_train), dtype=torch.float)\n",
    "before = X_train_scaled.clone().detach()\n",
    "X_test_scaled = torch.tensor(min_max_scaler.transform(X_test), dtype=torch.float)\n",
    "\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.float)\n",
    "y_test = torch.tensor(y_test.values, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RadialBasisGP(gpytorch.models.ExactGP, botorch.models.gpytorch.GPyTorchModel):\n",
    "    num_outputs = 1\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(RadialBasisGP, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(\n",
    "                ard_num_dims=train_x.shape[1]\n",
    "                )\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "# nlpd = gpytorch.metrics.negative_log_predictive_density(predictive_dist, y_test)\n",
    "# mse = gpytorch.metrics.mean_squared_error(predictive_dist, y_test, squared=True)\n",
    "# mae = gpytorch.metrics.mean_absolute_error(predictive_dist, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the domain bounds\n",
    "\n",
    "Setting the bounds for the domain (search space) requires prior knowledge about the problem. It may not be desireable to set the lower and upper bound to the empirical min and max if it is possible to have values outside of these ranges. Doing so may restrict the domain, and thus the co-domain. Additionally, bounds may be set that are according to technical specifications. For example, belt speed, cutting angle, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_max bounds\n",
    "bounds = torch.tensor(\n",
    "    [\n",
    "        [-2, -2, -2, -2, -2, -2, -2, -2],\n",
    "        [2, 2, 2, 2, 2, 2, 2, 2],\n",
    "    ],\n",
    "    dtype=torch.float,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Probability of Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BayesOpt: minimize the objective function --> .min()\n",
    "num_iters = 100\n",
    "num_queries = 10\n",
    "for i in range(num_queries):\n",
    "    print(\"-\"*20)\n",
    "    print(f\"iteration: {i}\")\n",
    "    print(f\"incumbent: {X_train_scaled[y_train.argmin()]}, obj. func. value: {y_train.min():.4f}\")\n",
    "\n",
    "    model, likelihood, loss_hist = fit_model(RadialBasisGP, num_iters)\n",
    "\n",
    "    policy = botorch.acquisition.analytic.ProbabilityOfImprovement(\n",
    "            model, best_f=y_train.min()\n",
    "        )\n",
    "\n",
    "    next_x, acq_score = botorch.optim.optimize_acqf(\n",
    "        policy,\n",
    "        bounds=bounds,\n",
    "        q=1,\n",
    "        num_restarts=20*8,\n",
    "        raw_samples=50*8\n",
    "    )\n",
    "\n",
    "    # evaluate the objective function and update training data\n",
    "    with torch.no_grad():\n",
    "        predictive_dist = likelihood(model(next_x))\n",
    "        next_y_mean = predictive_dist.mean\n",
    "        # predictive_upper, predictive_lower = predictive_dist.confidence_region()\n",
    "    \n",
    "    X_train_scaled = torch.cat([X_train_scaled, next_x])\n",
    "    y_train = torch.cat([y_train, next_y_mean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 3))\n",
    "plt.plot(torch.arange(len(loss_hist)), loss_hist)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Marginal Log-Likelihood\")\n",
    "plt.title(f\"Loss: {loss_hist[-1]:.4f}\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimizing_features = pd.DataFrame(X_train_scaled.numpy()[y_train.argmin()]).T\n",
    "minimizing_features.columns = list(X_train.columns)\n",
    "minimizing_features[\"objective_value\"] = y_train.min().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimizing_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Probability of Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BayesOpt: minimize the objective function --> .min()\n",
    "num_queries = 20\n",
    "batch_size = 4\n",
    "iters = num_queries // batch_size\n",
    "for i in range(num_queries):\n",
    "    print(\"-\"*20)\n",
    "    print(f\"iteration: {i}\")\n",
    "    print(f\"incumbent: {X_train_scaled[y_train.argmin()]}, obj. func. value: {y_train.min():.4f}\")\n",
    "\n",
    "    model, likelihood, loss_hist = fit_model(RadialBasisGP)\n",
    "\n",
    "    policy = botorch.acquisition.monte_carlo.qProbabilityOfImprovement(\n",
    "            model, best_f=y_train.min()\n",
    "        )\n",
    "\n",
    "    next_x, acq_score = botorch.optim.optimize_acqf(\n",
    "        policy,\n",
    "        bounds=bounds,\n",
    "        q=batch_size,\n",
    "        num_restarts=20*8,\n",
    "        raw_samples=50*8\n",
    "    )\n",
    "\n",
    "    # evaluate the objective function and update training data\n",
    "    with torch.no_grad():\n",
    "        predictive_dist = likelihood(model(next_x))\n",
    "        next_y_mean = predictive_dist.mean\n",
    "        # predictive_upper, predictive_lower = predictive_dist.confidence_region()\n",
    "    \n",
    "    X_train_scaled = torch.cat([X_train_scaled, next_x])\n",
    "    y_train = torch.cat([y_train, next_y_mean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 3))\n",
    "plt.plot(torch.arange(len(loss_hist)), loss_hist)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Marginal Log-Likelihood\")\n",
    "plt.title(f\"Loss: {loss_hist[-1]:.4f}\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimizing_features = pd.DataFrame(X_train_scaled.numpy()[y_train.argmin()]).T\n",
    "minimizing_features.columns = list(X_train.columns)\n",
    "minimizing_features[\"objective_value\"] = y_train.min().item()\n",
    "minimizing_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential constrained optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.fit import fit_gpytorch_model\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from botorch.acquisition import ExpectedImprovement, ConstrainedExpectedImprovement\n",
    "from botorch.optim import optimize_acqf\n",
    "from botorch.utils import standardize\n",
    "\n",
    "# Define the objective function (placeholder)\n",
    "def objective_function(X):\n",
    "    # Placeholder for the actual objective function\n",
    "    return -torch.sum(X ** 2, dim=-1, keepdim=True)\n",
    "\n",
    "# Generate some training data\n",
    "train_X = torch.rand(10, 2, dtype=torch.double)\n",
    "train_Y = objective_function(train_X)\n",
    "train_Y = standardize(train_Y)  # standardize the output\n",
    "\n",
    "# Define the Gaussian Process model\n",
    "gp_model = SingleTaskGP(train_X, train_Y)\n",
    "\n",
    "# Fit the GP model\n",
    "mll = ExactMarginalLogLikelihood(gp_model.likelihood, gp_model)\n",
    "fit_gpytorch_model(mll)\n",
    "\n",
    "# Initialize the acquisition function with the outcome constraint\n",
    "constraints = {0: (0.0, None)}\n",
    "best_f = train_Y.max()\n",
    "acq_function = ConstrainedExpectedImprovement(model=gp_model, best_f=best_f, objective_index=0, constraints=constraints)\n",
    "\n",
    "# Optimize the acquisition function with constraints\n",
    "bounds = torch.tensor([[0.0, 0.0], [1.0, 1.0]])\n",
    "candidates, _ = optimize_acqf(\n",
    "    acq_function=acq_function,\n",
    "    bounds=bounds,\n",
    "    q=1,\n",
    "    num_restarts=5,\n",
    "    raw_samples=20\n",
    ")\n",
    "\n",
    "print(\"Suggested candidate(s):\")\n",
    "print(candidates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality of grid predictions\n",
    "\n",
    "Computational complexity $\\mathcal{O}$ depends on the number of dimensions and the grandularity of the linspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_x = torch.linspace(0, 1, 50)\n",
    "grid_x1, grid_x2 = torch.meshgrid(grid_x, grid_x, indexing=\"ij\")\n",
    "xs = torch.vstack([grid_x1.flatten(), grid_x2.flatten()]).transpose(-1, -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_x1.shape, grid_x2.shape, xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "50**2 # 2,500 rows and 2 dimensions (columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_x = torch.linspace(0, 1, 10) # ONLY 10 points\n",
    "grid_1, grid_2, grid_3, grid_4, grid_5, grid_6, grid_7, grid_8 = torch.meshgrid(\n",
    "    [grid_x, grid_x, grid_x, grid_x, grid_x, grid_x, grid_x, grid_x], \n",
    "    indexing='ij'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_1.shape, grid_1.flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10**8 # 100 million rows and 8 dimensions (columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayesopt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
